1. My features tended to land into a few distinct categories. 
They either dealt with the letters used, combos used, or some distrubution 
found within the sentence. For example, I choose to check if the most common 
letter was e, t, a, o, and i as those were within the top 5 most used letters
for both lanaguages. Likewise, I checked if the senetence had any character that 
contained accents as english words rarely did and when they did they tended to 
just be symbols such as degrees. 

With this in mind I also checked suffixs and commonly put together letters
such as englishes "th" and dutches "hein", "sch", and "ij". 
Interestingly when searching for the existence of dutch suffixes my error increased
drastically but when searching for the lack of suffixes it drastically
reduced the error. 

I also noticed some patterns within the dutch language, for example they 
tended to have repeat letters more then english and there words ended in 
"t" much more then english words.

Finally I choosed a variety of proportion attributes such as 
their distrubution of vowels and average word length. My hope was that
their distrubution of letter usages would pop out here and be detected. 
I attempted to do the same with consonants but with inverse effects to my error. 

The last attribute I asked about was that of formality. From what I researched 
dutch tends to use less formal senetnces in the sense that symbols such that
";", ":", and so on appear less then in english. Thus I created a method that
counted the number of marks with rarer symbols being weighted more. 

2. For both my models I mostly just played around with a high amount of attribute,
creating ranges of attributes such as with average word length having three categories of 
range. After that I slowly tweeked my proportions and counts until I couldnt decrease the error
anymore. Finally I began to remove attributes one by one seeing which ones increased the error 
and which ones did so drastically. For some of the attributes I found by simply 
negating its feature (turning a < to a >) it would drastically reduce error in place
of increasing it. 

When it came to deciding the max depth, I ran every possible depth and stored
their error, plotting it on the graph found in testing_vs_training.png. 
I decided on a max depth of 20 since that was the turning point for the 
test error. 

3. For ada boosting I went through the same process of tweeking attributes. 
Likewise, I find the error generated by using a K value between 0 and 100 and 
then plotted it in the graph on testing_vs_training.png. I found after about k = 10
the decrease in error became non existent. Thus for the sake of speed I decided
on using a K = 50 for training. 

4. For both of my plots, I used about 70% of my data for training and 30%
for testing. 